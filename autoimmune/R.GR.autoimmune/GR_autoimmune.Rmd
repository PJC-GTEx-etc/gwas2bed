---
title: "Genetic and epigenetic fine mapping of causal autoimmune disease variants"
author: "Mikhail Dozmorov"
date: "November 1, 2014"
output: html_document
---
```{r setup, echo=FALSE}
source("utils.R")
suppressMessages(library(Hmisc)) # For rcorr function
suppressMessages(library(gplots))
suppressMessages(library(Biobase))
suppressMessages(library(limma))
library(reshape2)
#library(qvalue)
# Set up the environment
library(knitr) 
opts_chunk$set(cache.path='cache/', fig.path='img/', cache=F, tidy=T, fig.keep='high', echo=F, dpi=300, out.width=700)
options(replace.assign=TRUE, width=120)
suppressMessages(library(pander))
panderOptions('table.split.table', Inf)
set.seed(1)
```

[Genetic and epigenetic fine mapping of causal autoimmune disease variants](http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature13835.pdf)

[Supplemental table 1](http://www.nature.com/nature/journal/vaop/ncurrent/extref/nature13835-s1.xls) has genomic coordinates of disease-associated SNPs.

Analysis of TFBSs
===
Out of all regulatory datasets, we select only TFBSs.

```{r loadData1, echo=FALSE}
# Define output and data subfolders to use, change to analyze different data
rname<-"results//" # Output folder
# One or more GenomeRunner Web results data folders.
dname <- "data.gr//ENCODE_FDR/"
mtx<-do.call("rbind", lapply(dname, function(fn) as.matrix(read.table(paste(fn, "matrix.txt", sep=""), sep="\t", header=T, row.names=1))))
mtx <- mtx[grep("tfbs", rownames(mtx), ignore.case=T), ] # Limit the GFs to TFBSs and Histone marks
# Exploratory: check quantiles and remove diseaases showing no enrichments
# mtx.sumstat <- as.data.frame(apply(mtx, 2, quantile)) # Get quantiles
# mtx <- mtx[ , apply(mtx.sumstat, 2, function(x) sum(abs(x)) != 5)] # REmove those that have all "1" or "-1"
# Optional: filter unused genomic features
# mtx<-mtx[grep("snp", rownames(mtx), ignore.case=T, invert=T), ]
mtx<-mtx.transform(mtx) # -log10 transform p-values
# Optional: adjust columns for multiple testing. See utils.R for the function definition.
# mtx<-mtx.adjust(mtx) 
trackDb.hg19 <- read.table("data.gr//gf_descriptions.hg19.txt", sep="\t", row.names=1)
# Define file names for results output
fn_maxmin <- "results//maxmin_correlations_tfbs.txt"
fn_clust <- "results/clustering_tfbs.txt"
fn_degs <- "results/clusters-degs_tfbs.txt"
```

```{r preprocessData1, echo=FALSE}
dim(mtx) # Check original dimensions
# Define minimum number of times a row/col should have values above the cutoffs
numofsig<-1
cutoff<- -log10(0.1) # q-value significance cutoff
# What remains if we remove rows/cols with nothing significant
dim(mtx[apply(mtx, 1, function(x) sum(abs(x)>cutoff))>=numofsig, 
        apply(mtx, 2, function(x) sum(abs(x)>cutoff))>=numofsig])
# Trim the matrix
mtx<-mtx[apply(mtx, 1, function(x) sum(abs(x)>cutoff))>=numofsig, 
         apply(mtx, 2, function(x) sum(abs(x)>cutoff))>=numofsig]
```

```{r preprocessCorrel1, echo=FALSE}
# rcorr returns a list, [[1]] - correl coeffs, [[3]] - p-values. Type - pearson/spearman
mtx.cor<-rcorr(as.matrix(mtx), type="spearman")
# Optionally, try kendall correlation
# mtx.cor[[1]]<-cor(as.matrix(mtx), method="kendall")
```

```{r epigenomicVisualization1, echo=FALSE}
par(oma=c(5,0,0,5), mar=c(10, 4.1, 4.1, 5)) # Adjust margins
color<-colorRampPalette(c("blue","yellow")) # Define color gradient
#color<-greenred #Standard green-black-red palette
# Adjust clustering parameters.
# Distance: "euclidean", "maximum","manhattan" or "minkowski". Do not use "canberra" or "binary"
# Clustering: "ward", "single", "complete", "average", "mcquitty", "median" or "centroid"
dist.method<-"euclidean"  
hclust.method<-"ward.D2"
# Setting breaks to go from minimum to maximum correlation coefficients,
# excluding min/max outliers. This way we get rid of diagonale of 1's
granularity = 10
my.breaks <- seq(min(mtx.cor[[1]][mtx.cor[[1]]!=min(mtx.cor[[1]])]),
                 max(mtx.cor[[1]][mtx.cor[[1]]!=max(mtx.cor[[1]])]),
                 length.out=(2*granularity + 1))
h<-heatmap.2(as.matrix(mtx.cor[[1]]), trace="none", density.info="none", col=color, distfun=function(x){dist(x, method=dist.method)}, hclustfun=function(x){hclust(x, method=hclust.method)}, cexRow=0.7, cexCol=0.7, breaks=my.breaks)
# write.table(melt(mtx.cor[[1]][h$rowInd, h$colInd]), "results/tumorportal.cor.txt", sep="\t", quote=F, row.names=F, col.names=F)
```

**Text mining question 1:** Are the diseases within a cluster share stronger literature similarity than the diseases between the clusters? To answer, we need literature similarity scores for each pair, then split the pairs into cluster-specific groups and compare score distributions with what can be expected by chance, calculating the p-values for it. *Expected answer:* Diseases within each cluster are related to each other by literature findings stronger than could be expected by chance. Diseases between the clusters are not related to each other by literature findings, and this also may be statistically significant.

The top 10 pairs of disease-associated SNPs are most similar with each other.

```{r maxMin1, echo=FALSE}
# Checking max/min correlations
mtx.cor1<-mtx.cor[[1]]
diag(mtx.cor1)<-0 # We don't need to consider self correlations, zero them out
mtx.cor1[lower.tri(mtx.cor1)] <- 0 # Also zero out one matrix triangle, to avoid duplicate pairs
mtx.maxMin <- melt(mtx.cor1) # Convert the matrix into tidy data
mtx.maxMin <- mtx.maxMin[order(mtx.maxMin$value, decreasing=T), ] # Reorder the data by maxMin correlation
mtx.maxMin <- mtx.maxMin[mtx.maxMin$value != 0, ]
row.names(mtx.maxMin) <- NULL
colnames(mtx.maxMin) <- c("Disease 1", "Disease 2", "Corr. coefficient")
pander(head(mtx.maxMin, n=10))
write.table(mtx.maxMin, fn_maxmin, sep="\t", row.names=F)
```

The similarity dendrogram can be divided into separate groups:

```{r defineClusters1, echo=FALSE}
par(oma=c(0, 0, 0, 0), mar=c(5.1, 4.1, 4.1,25.1), cex=0.5)
# Plot the dendrogram only, limit y axis. attr(h$colDendrogram, "height") has the maximum height of the dendrogram.
plot(h$colDendrogram, horiz=T) 
# Cut the dentrogram into separate clusters. Tweak the height
abline(v=4) # Visually evaluate the height where to cut
c<-cut(h$colDendrogram, h=4) 
# Check the number of clusters, and the number of members.
for (i in 1:length(c$lower)){
  cat(paste("Cluster", formatC(i, width=2, flag="0"), sep=""), "has", formatC(attr(c$lower[[i]], "members"), width=3), "members", "\n")
  cat(paste(t(labels(c$lower[[i]])), collapse="\n"))
  cat(paste("\n", "\n"))
}
# Output the results into a file
unlink(fn_clust)
for (i in 1:length(c$lower)){ 
  write.table(paste(i, t(labels(c$lower[[i]])), sep="\t"), fn_clust, sep="\t", col.names=F, row.names=F, append=T)
}
```

The "Enrichment 1/2" columns show the average p-values of the group-specific SNPs-regulatory associations. A "-" sign indicates that an association is underrepresented. The "p-value" column shows whether the difference in the associations between the groups is statistically significantly different.

```{r defineGroups1, echo=FALSE}
eset.labels<-character() # Empty vector to hold cluster labels
eset.groups<-numeric() # Empty vector to hold cluster groups
# Set the minimum number of members to be considered for the differential analysis
minmembers<-3
for (i in 1:length(c$lower)) { # Go through each cluster
  # If the number of members is more than a minimum number of members
  if (attr(c$lower[[i]], "members") > minmembers) { 
    eset.labels<-append(eset.labels, labels(c$lower[[i]]))
    eset.groups<-append(eset.groups, rep(i, length(labels(c$lower[[i]]))))
  }
}
```

```{r limmaOnClusters1, warning=FALSE}
eset<-new("ExpressionSet", exprs=as.matrix(mtx[, eset.labels]))
# Make model matrix
design<-model.matrix(~ 0+factor(eset.groups)) 
colnames(design)<-paste("c", unique(eset.groups), sep="")
# Create a square matrix of counts of DEGs
degs.matrix<-matrix(0, length(c$lower), length(c$lower))
colnames(degs.matrix)<-paste("c", seq(1,length(c$lower)), sep="")
rownames(degs.matrix)<-paste("c", seq(1, length(c$lower)), sep="") 
# Tweak p-value and log2 fold change cutoffs
cutoff.pval<-0.05
cutoff.lfc<-log2(1)
unlink(fn_degs)
for(i in colnames(design)){ 
  for(j in colnames(design)){
    # Test only unique pairs of clusters
    if (as.numeric(sub("c", "", i)) < as.numeric(sub("c", "", j))) {
      # Contrasts between two clusters
      contrast.matrix<-makeContrasts(contrasts=paste(i, j, sep="-"), levels=design)
      fit <- lmFit(eset, design) 
      fit2 <- contrasts.fit(fit, contrast.matrix)
      fit2 <- eBayes(fit2)
      degs<-topTable(fit2, number=dim(exprs(eset))[[1]], adjust.method="BH") # , p.value=cutoff.pval, lfc=cutoff.lfc)
      if(dim(degs)[[1]]>0) {
        ndegs <- dim(degs[degs$logFC > cutoff.lfc & degs$adj.P.Val < cutoff.pval, ])[[1]]
        print(paste(i, "vs.", j, ", number of degs significant at adj.p.val<0.5:", ndegs))
        # Keep the number of DEGs in the matrix
        degs.matrix[as.numeric(sub("c", "", i)), as.numeric(sub("c", "", j))] <- ndegs
        if(ndegs > 0) {
          # Average values in clusters i and j
          i.av<-rowMeans(matrix(exprs(eset)[rownames(degs), eset.groups == as.numeric(sub("c", "", i))], nrow=dim(degs)[[1]]))
          j.av<-rowMeans(matrix(exprs(eset)[rownames(degs), eset.groups == as.numeric(sub("c", "", j))], nrow=dim(degs)[[1]]))
          # Merge and convert the values
          degs.pvals.log <- cbind(i.av, j.av)
          degs.pvals <- matrix(0, nrow=nrow(degs.pvals.log), ncol=ncol(degs.pvals.log), dimnames=list(rownames(degs.pvals.log), c(i, j))) # Empty matrix to hold converted p
          for (ii in 1:nrow(degs.pvals.log)) {
            for (jj in 1:ncol(degs.pvals.log)) {
              if (degs.pvals.log[ii, jj] < 0) {sign = -1} else {sign = 1}
              degs.pvals[ii, jj] <- sign/10^abs(degs.pvals.log[ii, jj])
            }
          }
          degs <- cbind(degs, degs.pvals) # Bind the differences p-values with the converted averaged association p-values
          degs <- degs[order(degs$adj.P.Val, decreasing=F), ] # Order them by the ratio of the differences
          degs.table <- merge(degs, trackDb.hg19, by="row.names", all.x=TRUE, sort=FALSE) # Merge with the descriptions
          write.table(degs.table[1:ndegs, c(1, 10, 8, 9, 6, 5)], fn_degs, sep="\t", col.names=NA, append=T)
          if(ndegs > 20) {
            ndegs <- 20
          }
          pander(degs.table[1:ndegs, c(1, 10, 8, 9, 6)])
          # Put it all together in a file, keeping columns with average transformed p-value being significant in at least one condition
         
        }
      }
    }
  }
}
print("Counts of regulatory elements differentially associated with each group")
pander(degs.matrix)
```

**Text mining question 2:** Are the terms associated stronger with the diseases in one vs. the other cluster based on the literature strength? Are the terms themselves related based on the literature? *Expected answer:* Yes, the literature associations should confirm the relationships.

Summary
---
1. There are 4 clusters. The first cluster drives all the differences.

|    | C1 | C2                                                      | C3                                                       | C4                                                       |
|----|----|---------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|
| C1 |    | Cell types: Gm12878 Reg: NFkB, Pol2, MTA3, NFIC, NFATC1 | Cell types: Gm12878  Reg: NFkB, Pol2, MTA3, NFIC, NFATC1 | Cell types: Gm12878  Reg: NFkB, Pol2, MTA3, NFIC, NFATC1 |
| C2 |    |                                                         | Nothing significant                                      | Nothing significant                                      |
| C3 |    |                                                         |                                                          | Nothing significant                                      |
| C4 |    |                                                         |                                                          |                                                          |

Analysis of histone marks
===
Out of all regulatory datasets, we select only histone marks

```{r loadData2, echo=FALSE}
# Define output and data subfolders to use, change to analyze different data
rname<-"results//" # Output folder
# One or more GenomeRunner Web results data folders.
dname <- "data.gr//ENCODE_FDR/"
mtx<-do.call("rbind", lapply(dname, function(fn) as.matrix(read.table(paste(fn, "matrix.txt", sep=""), sep="\t", header=T, row.names=1))))
mtx <- mtx[grep("histone", rownames(mtx), ignore.case=T), ] # Limit the GFs to TFBSs and Histone marks
# Exploratory: check quantiles and remove diseaases showing no enrichments
# mtx.sumstat <- as.data.frame(apply(mtx, 2, quantile)) # Get quantiles
# mtx <- mtx[ , apply(mtx.sumstat, 2, function(x) sum(abs(x)) != 5)] # REmove those that have all "1" or "-1"
# Optional: filter unused genomic features
# mtx<-mtx[grep("snp", rownames(mtx), ignore.case=T, invert=T), ]
mtx<-mtx.transform(mtx) # -log10 transform p-values
# Optional: adjust columns for multiple testing. See utils.R for the function definition.
# mtx<-mtx.adjust(mtx) 
trackDb.hg19 <- read.table("data.gr//gf_descriptions.hg19.txt", sep="\t", row.names=1)
fn_maxmin <- "results//maxmin_correlations_histone.txt"
fn_clust <- "results/clustering_histone.txt"
fn_degs <- "results/clusters-degs_histone.txt"
```

```{r preprocessData2, echo=FALSE}
dim(mtx) # Check original dimensions
# Define minimum number of times a row/col should have values above the cutoffs
numofsig<-1
cutoff<- -log10(0.1) # q-value significance cutoff
# What remains if we remove rows/cols with nothing significant
dim(mtx[apply(mtx, 1, function(x) sum(abs(x)>cutoff))>=numofsig, 
        apply(mtx, 2, function(x) sum(abs(x)>cutoff))>=numofsig])
# Trim the matrix
mtx<-mtx[apply(mtx, 1, function(x) sum(abs(x)>cutoff))>=numofsig, 
         apply(mtx, 2, function(x) sum(abs(x)>cutoff))>=numofsig]
```

```{r preprocessCorrel2, echo=FALSE}
# rcorr returns a list, [[1]] - correl coeffs, [[3]] - p-values. Type - pearson/spearman
mtx.cor<-rcorr(as.matrix(mtx), type="spearman")
# Optionally, try kendall correlation
# mtx.cor[[1]]<-cor(as.matrix(mtx), method="kendall")
```

```{r epigenomicVisualization2, echo=FALSE}
par(oma=c(5,0,0,5), mar=c(10, 4.1, 4.1, 5)) # Adjust margins
color<-colorRampPalette(c("blue","yellow")) # Define color gradient
#color<-greenred #Standard green-black-red palette
# Adjust clustering parameters.
# Distance: "euclidean", "maximum","manhattan" or "minkowski". Do not use "canberra" or "binary"
# Clustering: "ward", "single", "complete", "average", "mcquitty", "median" or "centroid"
dist.method<-"euclidean"  
hclust.method<-"ward.D2"
# Setting breaks to go from minimum to maximum correlation coefficients,
# excluding min/max outliers. This way we get rid of diagonale of 1's
granularity = 10
my.breaks <- seq(min(mtx.cor[[1]][mtx.cor[[1]]!=min(mtx.cor[[1]])]),
                 max(mtx.cor[[1]][mtx.cor[[1]]!=max(mtx.cor[[1]])]),
                 length.out=(2*granularity + 1))
h<-heatmap.2(as.matrix(mtx.cor[[1]]), trace="none", density.info="none", col=color, distfun=function(x){dist(x, method=dist.method)}, hclustfun=function(x){hclust(x, method=hclust.method)}, cexRow=0.7, cexCol=0.7, breaks=my.breaks)
```

**Text mining question 1:** Are the diseases within a cluster share stronger literature similarity than the diseases between the clusters? To answer, we need literature similarity scores for each pair, then split the pairs into cluster-specific groups and compare score distributions with what can be expected by chance, calculating the p-values for it. *Expected answer:* Diseases within each cluster are related to each other by literature findings stronger than could be expected by chance. Diseases between the clusters are not related to each other by literature findings, and this also may be statistically significant.

The top 10 pairs of autoimmune-associated SNPs are most similar with each other.

```{r maxMin2, echo=FALSE}
# Checking max/min correlations
mtx.cor1<-mtx.cor[[1]]
diag(mtx.cor1)<-0 # We don't need to consider self correlations, zero them out
mtx.cor1[lower.tri(mtx.cor1)] <- 0 # Also zero out one matrix triangle, to avoid duplicate pairs
mtx.maxMin <- melt(mtx.cor1) # Convert the matrix into tidy data
mtx.maxMin <- mtx.maxMin[order(mtx.maxMin$value, decreasing=T), ] # Reorder the data by maxMin correlation
mtx.maxMin <- mtx.maxMin[mtx.maxMin$value != 0, ]
row.names(mtx.maxMin) <- NULL
colnames(mtx.maxMin) <- c("Disease 1", "Disease 2", "Corr. coefficient")
pander(head(mtx.maxMin, n=10))
write.table(mtx.maxMin, fn_maxmin, sep="\t", row.names=F)
```

The similarity dendrogram can be divided into separate groups:

```{r defineClusters2, echo=FALSE}
par(oma=c(0, 0, 0, 0), mar=c(5.1, 4.1, 4.1,25.1), cex=0.5)
# Plot the dendrogram only, limit y axis. attr(h$colDendrogram, "height") has the maximum height of the dendrogram.
plot(h$colDendrogram, horiz=T) 
# Cut the dentrogram into separate clusters. Tweak the height
abline(v=2.8) # Visually evaluate the height where to cut
c<-cut(h$colDendrogram, h=2.8) 
# Check the number of clusters, and the number of members.
for (i in 1:length(c$lower)){
  cat(paste("Cluster", formatC(i, width=2, flag="0"), sep=""), "has", formatC(attr(c$lower[[i]], "members"), width=3), "members", "\n")
  cat(paste(t(labels(c$lower[[i]])), collapse="\n"))
  cat(paste("\n", "\n"))
}
# Output the results into a file
unlink(fn_clust)
for (i in 1:length(c$lower)){ 
  write.table(paste(i, t(labels(c$lower[[i]])), sep="\t"), fn_clust, sep="\t", col.names=F, row.names=F, append=T)
}
```

The "Enrichment 1/2" columns show the average p-values of the group-specific SNPs-regulatory associations. A "-" sign indicates that an association is underrepresented. The "p-value" column shows whether the difference in the associations bwtween the groups is statistically significantly different.

```{r defineGroups2, echo=FALSE}
eset.labels<-character() # Empty vector to hold cluster labels
eset.groups<-numeric() # Empty vector to hold cluster groups
# Set the minimum number of members to be considered for the differential analysis
minmembers<-3
for (i in 1:length(c$lower)) { # Go through each cluster
  # If the number of members is more than a minimum number of members
  if (attr(c$lower[[i]], "members") > minmembers) { 
    eset.labels<-append(eset.labels, labels(c$lower[[i]]))
    eset.groups<-append(eset.groups, rep(i, length(labels(c$lower[[i]]))))
  }
}
```

```{r limmaOnClusters2, warning=FALSE}
eset<-new("ExpressionSet", exprs=as.matrix(mtx[, eset.labels]))
# Make model matrix
design<-model.matrix(~ 0+factor(eset.groups)) 
colnames(design)<-paste("c", unique(eset.groups), sep="")
# Create a square matrix of counts of DEGs
degs.matrix<-matrix(0, length(c$lower), length(c$lower))
colnames(degs.matrix)<-paste("c", seq(1,length(c$lower)), sep="")
rownames(degs.matrix)<-paste("c", seq(1, length(c$lower)), sep="") 
# Tweak p-value and log2 fold change cutoffs
cutoff.pval<-0.05
cutoff.lfc<-log2(1)
unlink(fn_degs)
for(i in colnames(design)){ 
  for(j in colnames(design)){
    # Test only unique pairs of clusters
    if (as.numeric(sub("c", "", i)) < as.numeric(sub("c", "", j))) {
      # Contrasts between two clusters
      contrast.matrix<-makeContrasts(contrasts=paste(i, j, sep="-"), levels=design)
      fit <- lmFit(eset, design) 
      fit2 <- contrasts.fit(fit, contrast.matrix)
      fit2 <- eBayes(fit2)
      degs<-topTable(fit2, number=dim(exprs(eset))[[1]], adjust.method="BH") # , p.value=cutoff.pval, lfc=cutoff.lfc)
      if(dim(degs)[[1]]>0) {
        ndegs <- dim(degs[degs$logFC > cutoff.lfc & degs$adj.P.Val < cutoff.pval, ])[[1]]
        print(paste(i, "vs.", j, ", number of degs significant at adj.p.val<0.5:", ndegs))
        # Keep the number of DEGs in the matrix
        degs.matrix[as.numeric(sub("c", "", i)), as.numeric(sub("c", "", j))] <- ndegs
        if(ndegs > 0) {
          # Average values in clusters i and j
          i.av<-rowMeans(matrix(exprs(eset)[rownames(degs), eset.groups == as.numeric(sub("c", "", i))], nrow=dim(degs)[[1]]))
          j.av<-rowMeans(matrix(exprs(eset)[rownames(degs), eset.groups == as.numeric(sub("c", "", j))], nrow=dim(degs)[[1]]))
          # Merge and convert the values
          degs.pvals.log <- cbind(i.av, j.av)
          degs.pvals <- matrix(0, nrow=nrow(degs.pvals.log), ncol=ncol(degs.pvals.log), dimnames=list(rownames(degs.pvals.log), c(i, j))) # Empty matrix to hold converted p
          for (ii in 1:nrow(degs.pvals.log)) {
            for (jj in 1:ncol(degs.pvals.log)) {
              if (degs.pvals.log[ii, jj] < 0) {sign = -1} else {sign = 1}
              degs.pvals[ii, jj] <- sign/10^abs(degs.pvals.log[ii, jj])
            }
          }
          degs <- cbind(degs, degs.pvals) # Bind the differences p-values with the converted averaged association p-values
          degs <- degs[order(degs$adj.P.Val, decreasing=F), ] # Order them by the ratio of the differences
          degs.table <- merge(degs, trackDb.hg19, by="row.names", all.x=TRUE, sort=FALSE) # Merge with the descriptions
          write.table(degs.table[1:ndegs, c(1, 10, 8, 9, 6, 5)], fn_degs, sep="\t", col.names=NA, append=T)
          if(ndegs > 20) {
            ndegs <- 20
          }
          pander(degs.table[1:ndegs, c(1, 10, 8, 9, 6)])
          # Put it all together in a file, keeping columns with average transformed p-value being significant in at least one condition
         
        }
      }
    }
  }
}
print("Counts of regulatory elements differentially associated with each group")
pander(degs.matrix)
```

**Text mining question 2:** Are the terms associated stronger with the diseases in one vs. the other cluster based on the literature strength? Are the terms themselves related based on the literature? *Expected answer:* Yes, the literature associations should confirm the relationships.

Summary
---

1. Again, cluster 1 is strongly distinct. Cluster 2 is less so. Histone marks seem all active.

|    | C1 | C2                                                                                | C3                                                                                | C4                                                                                |
|----|----|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
| C1 |    | Cell types: Gm12878, CD20+  Reg: H3K4me1, H3K9me3, H3K9ac, H3K27ac, H2az, H3K4me2 | Cell types: Gm12878, CD20+  Reg: H3K4me1, H3K9me3, H3K9ac, H3K27ac, H2az, H3K4me2 | Cell types: Gm12878, CD20+  Reg: H3K4me1, H3K9me3, H3K9ac, H3K27ac, H2az, H3K4me2 |
| C2 |    |                                                                                   | Cell types: K562, NHEK, NHDF-Ad, NH-A, HMEC  Reg: H3K36me3, H4K20me1, H3K79me2    | Nothing significant                                                               |
| C3 |    |                                                                                   |                                                                                   | Nothing significant                                                               |
| C4 |    |                                                                                   |                                                                                   |                                                                                   |

Analysis of all regulatory datasets
===
Out of all regulatory datasets, we select all. The goal here is to get potentially tighter clustering.

```{r loadData3, echo=FALSE}
# Define output and data subfolders to use, change to analyze different data
rname<-"results//" # Output folder
# One or more GenomeRunner Web results data folders.
dname <- "data.gr//ENCODE_FDR/"
mtx<-do.call("rbind", lapply(dname, function(fn) as.matrix(read.table(paste(fn, "matrix.txt", sep=""), sep="\t", header=T, row.names=1))))
# mtx <- mtx[grep("histone", rownames(mtx), ignore.case=T), ] # Limit the GFs to TFBSs and Histone marks
# Exploratory: check quantiles and remove diseaases showing no enrichments
# mtx.sumstat <- as.data.frame(apply(mtx, 2, quantile)) # Get quantiles
# mtx <- mtx[ , apply(mtx.sumstat, 2, function(x) sum(abs(x)) != 5)] # REmove those that have all "1" or "-1"
# Optional: filter unused genomic features
# mtx<-mtx[grep("snp", rownames(mtx), ignore.case=T, invert=T), ]
mtx<-mtx.transform(mtx) # -log10 transform p-values
# Optional: adjust columns for multiple testing. See utils.R for the function definition.
# mtx<-mtx.adjust(mtx) 
trackDb.hg19 <- read.table("data.gr//gf_descriptions.hg19.txt", sep="\t", row.names=1)
fn_maxmin <- "results//maxmin_correlations_all.txt"
fn_clust <- "results/clustering_all.txt"
fn_degs <- "results/clusters-degs_all.txt"
```

```{r preprocessData3, echo=FALSE}
dim(mtx) # Check original dimensions
# Define minimum number of times a row/col should have values above the cutoffs
numofsig<-1
cutoff<- -log10(0.1) # q-value significance cutoff
# What remains if we remove rows/cols with nothing significant
dim(mtx[apply(mtx, 1, function(x) sum(abs(x)>cutoff))>=numofsig, 
        apply(mtx, 2, function(x) sum(abs(x)>cutoff))>=numofsig])
# Trim the matrix
mtx<-mtx[apply(mtx, 1, function(x) sum(abs(x)>cutoff))>=numofsig, 
         apply(mtx, 2, function(x) sum(abs(x)>cutoff))>=numofsig]
```

```{r preprocessCorrel3, echo=FALSE}
# rcorr returns a list, [[1]] - correl coeffs, [[3]] - p-values. Type - pearson/spearman
mtx.cor<-rcorr(as.matrix(mtx), type="spearman")
# Optionally, try kendall correlation
# mtx.cor[[1]]<-cor(as.matrix(mtx), method="kendall")
```

```{r epigenomicVisualization3, echo=FALSE}
par(oma=c(5,0,0,5), mar=c(10, 4.1, 4.1, 5)) # Adjust margins
color<-colorRampPalette(c("blue","yellow")) # Define color gradient
#color<-greenred #Standard green-black-red palette
# Adjust clustering parameters.
# Distance: "euclidean", "maximum","manhattan" or "minkowski". Do not use "canberra" or "binary"
# Clustering: "ward", "single", "complete", "average", "mcquitty", "median" or "centroid"
dist.method<-"euclidean"  
hclust.method<-"ward.D2"
# Setting breaks to go from minimum to maximum correlation coefficients,
# excluding min/max outliers. This way we get rid of diagonale of 1's
granularity = 10
my.breaks <- seq(min(mtx.cor[[1]][mtx.cor[[1]]!=min(mtx.cor[[1]])]),
                 max(mtx.cor[[1]][mtx.cor[[1]]!=max(mtx.cor[[1]])]),
                 length.out=(2*granularity + 1))
h<-heatmap.2(as.matrix(mtx.cor[[1]]), trace="none", density.info="none", col=color, distfun=function(x){dist(x, method=dist.method)}, hclustfun=function(x){hclust(x, method=hclust.method)}, cexRow=0.7, cexCol=0.7, breaks=my.breaks)
```

The top 10 pairs of disease-associated SNPs are most similar with each other.

```{r maxMin3, echo=FALSE}
# Checking max/min correlations
mtx.cor1<-mtx.cor[[1]]
diag(mtx.cor1)<-0 # We don't need to consider self correlations, zero them out
mtx.cor1[lower.tri(mtx.cor1)] <- 0 # Also zero out one matrix triangle, to avoid duplicate pairs
mtx.maxMin <- melt(mtx.cor1) # Convert the matrix into tidy data
mtx.maxMin <- mtx.maxMin[order(mtx.maxMin$value, decreasing=T), ] # Reorder the data by maxMin correlation
mtx.maxMin <- mtx.maxMin[mtx.maxMin$value != 0, ]
row.names(mtx.maxMin) <- NULL
colnames(mtx.maxMin) <- c("Disease 1", "Disease 2", "Corr. coefficient")
pander(head(mtx.maxMin, n=10))
write.table(mtx.maxMin, fn_maxmin, sep="\t", row.names=F)
```

The similarity dendrogram can be divided into separate groups:

```{r defineClusters3, echo=FALSE}
par(oma=c(0, 0, 0, 0), mar=c(5.1, 4.1, 4.1,25.1), cex=0.5)
# Plot the dendrogram only, limit y axis. attr(h$colDendrogram, "height") has the maximum height of the dendrogram.
plot(h$colDendrogram, horiz=T) 
# Cut the dentrogram into separate clusters. Tweak the height
abline(v=2.5) # Visually evaluate the height where to cut
c<-cut(h$colDendrogram, h=2.5) 
# Check the number of clusters, and the number of members.
for (i in 1:length(c$lower)){
  cat(paste("Cluster", formatC(i, width=2, flag="0"), sep=""), "has", formatC(attr(c$lower[[i]], "members"), width=3), "members", "\n")
  cat(paste(t(labels(c$lower[[i]])), collapse="\n"))
  cat(paste("\n", "\n"))
}
# Output the results into a file
unlink(fn_clust)
for (i in 1:length(c$lower)){ 
  write.table(paste(i, t(labels(c$lower[[i]])), sep="\t"), fn_clust, sep="\t", col.names=F, row.names=F, append=T)
}
```

The "Enrichment 1/2" columns show the average p-values of the group-specific SNPs-regulatory associations. A "-" sign indicates that an association is underrepresented. The "p-value" column shows whether the difference in the associations bwtween the groups is statistically significantly different.

```{r defineGroups3, echo=FALSE}
eset.labels<-character() # Empty vector to hold cluster labels
eset.groups<-numeric() # Empty vector to hold cluster groups
# Set the minimum number of members to be considered for the differential analysis
minmembers<-3
for (i in 1:length(c$lower)) { # Go through each cluster
  # If the number of members is more than a minimum number of members
  if (attr(c$lower[[i]], "members") > minmembers) { 
    eset.labels<-append(eset.labels, labels(c$lower[[i]]))
    eset.groups<-append(eset.groups, rep(i, length(labels(c$lower[[i]]))))
  }
}
```

```{r limmaOnClusters3, warning=FALSE}
eset<-new("ExpressionSet", exprs=as.matrix(mtx[, eset.labels]))
# Make model matrix
design<-model.matrix(~ 0+factor(eset.groups)) 
colnames(design)<-paste("c", unique(eset.groups), sep="")
# Create a square matrix of counts of DEGs
degs.matrix<-matrix(0, length(c$lower), length(c$lower))
colnames(degs.matrix)<-paste("c", seq(1,length(c$lower)), sep="")
rownames(degs.matrix)<-paste("c", seq(1, length(c$lower)), sep="") 
# Tweak p-value and log2 fold change cutoffs
cutoff.pval<-0.05
cutoff.lfc<-log2(1)
unlink(fn_degs)
for(i in colnames(design)){ 
  for(j in colnames(design)){
    # Test only unique pairs of clusters
    if (as.numeric(sub("c", "", i)) < as.numeric(sub("c", "", j))) {
      # Contrasts between two clusters
      contrast.matrix<-makeContrasts(contrasts=paste(i, j, sep="-"), levels=design)
      fit <- lmFit(eset, design) 
      fit2 <- contrasts.fit(fit, contrast.matrix)
      fit2 <- eBayes(fit2)
      degs<-topTable(fit2, number=dim(exprs(eset))[[1]], adjust.method="BH") # , p.value=cutoff.pval, lfc=cutoff.lfc)
      if(dim(degs)[[1]]>0) {
        ndegs <- dim(degs[degs$logFC > cutoff.lfc & degs$adj.P.Val < cutoff.pval, ])[[1]]
        print(paste(i, "vs.", j, ", number of degs significant at adj.p.val<0.5:", ndegs))
        # Keep the number of DEGs in the matrix
        degs.matrix[as.numeric(sub("c", "", i)), as.numeric(sub("c", "", j))] <- ndegs
        if(ndegs > 0) {
          # Average values in clusters i and j
          i.av<-rowMeans(matrix(exprs(eset)[rownames(degs), eset.groups == as.numeric(sub("c", "", i))], nrow=dim(degs)[[1]]))
          j.av<-rowMeans(matrix(exprs(eset)[rownames(degs), eset.groups == as.numeric(sub("c", "", j))], nrow=dim(degs)[[1]]))
          # Merge and convert the values
          degs.pvals.log <- cbind(i.av, j.av)
          degs.pvals <- matrix(0, nrow=nrow(degs.pvals.log), ncol=ncol(degs.pvals.log), dimnames=list(rownames(degs.pvals.log), c(i, j))) # Empty matrix to hold converted p
          for (ii in 1:nrow(degs.pvals.log)) {
            for (jj in 1:ncol(degs.pvals.log)) {
              if (degs.pvals.log[ii, jj] < 0) {sign = -1} else {sign = 1}
              degs.pvals[ii, jj] <- sign/10^abs(degs.pvals.log[ii, jj])
            }
          }
          degs <- cbind(degs, degs.pvals) # Bind the differences p-values with the converted averaged association p-values
          degs <- degs[order(degs$adj.P.Val, decreasing=F), ] # Order them by the ratio of the differences
          degs.table <- merge(degs, trackDb.hg19, by="row.names", all.x=TRUE, sort=FALSE) # Merge with the descriptions
          write.table(degs.table[1:ndegs, c(1, 10, 8, 9, 6, 5)], fn_degs, sep="\t", col.names=NA, append=T)
          if(ndegs > 20) {
            ndegs <- 20
          }
          pander(degs.table[1:ndegs, c(1, 10, 8, 9, 6)])
          # Put it all together in a file, keeping columns with average transformed p-value being significant in at least one condition
         
        }
      }
    }
  }
}
print("Counts of regulatory elements differentially associated with each group")
pander(degs.matrix)
```

Summary
---
The picture is not as good as when we are taking subsets of regulatory datasets.

Overlap analysis
===
```{r loadOverlapMtx}
mtx.overlap <- read.table("data/overlapMatrix.txt", sep="\t", head=F)
mtx.overlap.carpet <- dcast(mtx.overlap, V1~V2, mean)
rownames(mtx.overlap.carpet) <- mtx.overlap.carpet$V1
mtx.overlap.carpet <- as.matrix(mtx.overlap.carpet[, -1])
mtx.overlap$V1 <- sub(".bed", "", mtx.overlap$V1)
mtx.overlap$V2 <- sub(".bed", "", mtx.overlap$V2)
```

```{r visualizeOverlapMtx}
par(oma=c(5,0,0,5), mar=c(10, 4.1, 4.1, 5)) # Adjust margins
color<-colorRampPalette(c("blue","yellow")) # Define color gradient
#color<-greenred #Standard green-black-red palette
# Adjust clustering parameters.
# Distance: "euclidean", "maximum","manhattan" or "minkowski". Do not use "canberra" or "binary"
# Clustering: "ward", "single", "complete", "average", "mcquitty", "median" or "centroid"
dist.method<-"euclidean"  
hclust.method<-"ward.D2"
# Setting breaks to go from minimum to maximum correlation coefficients,
# excluding min/max outliers. This way we get rid of diagonale of 1's
h<-heatmap.2(mtx.overlap.carpet, trace="none", density.info="none", col=color, distfun=function(x){dist(x, method=dist.method)}, hclustfun=function(x){hclust(x, method=hclust.method)}, cexRow=0.7, cexCol=0.7, scale="row")
```

```{r}
a <- mtx.overlap[!(mtx.overlap[, 1] == mtx.overlap[, 2]), 3, drop=F]
rownames(a) <- paste(mtx.overlap[, 1], mtx.overlap[, 2], sep="-")
mtx.cor.melted <- melt(mtx.cor[[1]])
b <- mtx.cor.melted[!(mtx.cor.melted[, 1] == mtx.cor.melted[, 2]), 3, drop=F]
rownames(b) <- paste(mtx.cor.melted[, 1], mtx.cor.melted[, 2], sep="-")
b <- b[!(b[, 1] == b[, 2]), ]
c <- merge(a, b, by="row.names")
cor(c[, 2], c[, 3])
```

